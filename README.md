AttentionIsAllUNeed-PyTorch

This project contains a PyTorch implementation of the Transformer model architecture described in the paper "Attention Is All You Need" (https://arxiv.org/abs/1706.03762).

The implementation focuses on recreating the key components of the Transformer from scratch, including:

    Scaled dot-product attention
    Multi-head attention
    Position-wise feedforward layers
    Embedding layers
    Residual connections
    Layer normalization

The goal of this implementation is to provide a clean, well-commented PyTorch version of the Transformer model based solely on the details in the original paper, rather than using any pre-built transformer libraries.

It demonstrates how the Transformer can be implemented in PyTorch code following object-oriented design principles. The modular implementation makes it easy to modify and extend components for learning purposes.
